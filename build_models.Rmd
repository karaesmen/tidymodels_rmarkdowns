---
title: "Build a model"
author: "Ezgi Karaesmen"
date: "5/22/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r load, include = FALSE, eval=FALSE, message = FALSE, warning = FALSE}
library(readr)
library(rstanarm)
library(tidymodels)

pkgs <- c("tidymodels", "readr", "rstanarm")

theme_set(theme_bw() + theme(legend.position = "top"))
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

This is an accompanying RMardown document for the get started article [Build a model](https://www.tidymodels.org/start/models/) on [tidymodels.org](https://www.tidymodels.org/). 

We will walk through the steps of creating a statistical model using [parsnip package](https://tidymodels.github.io/parsnip/) from [tidymodels](https://tidymodels.tidymodels.org/).

## Load Libraries

```{r eval=TRUE}
library(tidymodels)  # for the parsnip package, along with the rest of tidymodels

# Helper packages
library(readr)       # for importing data
```


## The Sea Urchins Data

Let's use the data from [Constable (1993)](https://link.springer.com/article/10.1007/BF00349318) to explore how three different feeding regimes affect the size of sea urchins over time. The initial size of the sea urchins at the beginning of the experiment probably affects how big they grow as they are fed. 

To start, let's read our urchins data into R and modify so it's ready to use:

1. Import data with `readr::read_csv()` with a [url]("https://tidymodels.org/start/models/urchins.csv") where the CSV data is located. (See `?read::read_csv()` if you need help.)    
2. Change column names to define variables better.   
3. Convert `food_regime` column to factor, because factors are very helpful for modeling.

```{r data}
urchins <-
  read_csv("https://tidymodels.org/start/models/urchins.csv") %>% 
  # Change column names
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  # Convert to factor
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))
```

Let's take a quick look at the data:

```{r}
urchins
```

The urchins data is a `tibble`. For each of the `r nrow(urchins)` urchins, we know their:

+ experimental feeding regime group (`food_regime`: either `Initial`, `Low`, or `High`),
+ size in milliliters at the start of the experiment (`initial_volume`), and
+ suture width at the end of the experiment (`width`).

As a first step in modeling, it's always a good idea to plot the data: 

```{r urchin-plot}
ggplot(urchins,
       aes(x = initial_volume, 
           y = width, 
           group = food_regime, 
           col = food_regime)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
```

We can see that urchins that were larger in volume at the start of the experiment tended to have wider sutures at the end, but the slopes of the lines look different so this effect may depend on the feeding regime condition.

## Build and fit a model

To test our hypothesis, let's fit a standard two-way analysis of variance (ANOVA) model with the following model formula:

```{r two-way-int, eval = FALSE}
width ~ initial_volume * food_regime
```

This allows our regression model depending on initial volume to have separate slopes and intercepts for each food regime. 

We will use the `parsnip` package to create and fit our model.
Take a look at the `parsnip` vignette to familiarize yourself with the concepts:

```{r}
vignette("parsnip_Intro")
```


For this kind of model, ordinary least squares is a good initial approach. Since there is a numeric outcome and the model should be linear with slopes and intercepts, the **model type** is ["linear regression"](https://tidymodels.github.io/parsnip/reference/linear_reg.html).

With tidymodels, we start by specifying the _functional form_ of the model (**model type**) that we want using the [parsnip package](https://tidymodels.github.io/parsnip/).  We can declare this with: 


```{r lm-tm}
linear_reg()
```

Then we set an **engine** which is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. 

For example, to use ordinary least squares, we can set the engine to be `lm`:

```{r lm-spec}
linear_reg() %>% 
  set_engine("lm")
```

The [documentation page for `linear_reg()`](https://tidymodels.github.io/parsnip/reference/linear_reg.html) lists the possible engines. You can also access this document via R with:

```{r}
?linear_reg
```


Now that we defined our **model_type** and **engine**, we'll save this **model object** as `lm_mod`.

```{r}
lm_mod <- 
  linear_reg() %>% 
  set_engine("lm")
```

From here, the model can be estimated or trained using the [`fit()`](https://tidymodels.github.io/parsnip/reference/fit.html) function. Here, `fit()` takes three arguments:

1. we use pipe (`%>%`) to provide our model object `lm_mod`   
2. define a **formula** (`width ~ initial_volume * food_regime`)   
3. provide a dataset (`urchins`) to fit our model

Finally, we save this **fit object**

```{r lm-fit}
lm_fit <- 
  lm_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
```

We use `tidy()` function from **broom** package to see the results in a tidy data frame format:

```{r lm-table}
tidy(lm_fit)
```

## Use a model to predict {#predict-model}

This fitted object `lm_fit` has the `lm` model output built-in, which you can access with `lm_fit$fit`, but there are some benefits to using the fitted parsnip model object when it comes to predicting.

Let's make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for, to show in our graph:

```{r new-points}
new_points <- expand.grid(initial_volume = 20, 
                          food_regime = c("Initial", "Low", "High"))
new_points
```

To get our predicted results, we can use the `predict()` function to find the mean values at 20ml. 

If we had used `lm()` to fit the model directly, we could have used `predict.lm()` to easily get these values. However, if we decide to use a different model to estimate urchin size (_spoiler:_ we will!), it is likely that a completely different syntax (different arguments, objects, data processing etc.) would be required. 

Instead, with tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values. 

First, let's generate the predicted mean body width values: 

```{r lm-pred-mean}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

When making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format. 

Here we,    

1. Get confidence intervals by defining `type="conf_int"`     
2. Combine these confidence intervals with previously predicted mean body width values
3. Plot mean values and confidence intervals

```{r lm-all-pred}
conf_int_pred <- predict(lm_fit, 
                         new_data = new_points, 
                         type = "conf_int")
conf_int_pred

# Now combine: 
plot_data <- 
  new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(conf_int_pred)

# and plot:
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size")
```

Ta-da! Here we see predicted values of urchin width if the urchin had an initial volume of 20 and were grown in an environment with initial, low or high levels of nutrition.

See R documentation and other arguments and details for `predict()` function with:

```{r}
?predict.model_fit
```


## Model with a different engine

Let's leave the frequentist world behind and take a walk on the Bayesian (wild) side. 

To conduct a [Bayesian analysis](https://bayesian.org/what-is-bayesian-analysis/) a [_prior distribution_](https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7) needs to be declared for each model parameter that represents the possible values of the parameters. Here we will use a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom) for our priors.

Take a look at R help document for `linear_reg` and scroll down to **Details** to see available engines for this model type.

```{r}
?linear_reg
```

After skimming the R documentation, we can see that **Stan** (a probabilistic programming language for statistical inference written in C++ typically used for Bayesian analysis) is an available engine for our model type `linear_reg`.

To conduct a Bayesian analysis with tidymodels, we will take advantage of the `stan_glm()` function from **rstanarm** package. A list of arguments to define prior distributions for `stan_glm()` is available on [**rstanarm** documentation page](https://mc-stan.org/rstanarm/articles/priors.html). In our case, we have two function arguments that needs specification: `prior` and `prior_intercept`. We will define both of these arguments in the `set_engine()` since they're relevant to the specific engine we will be using.

We take the following steps:

1. Define our prior distribution with `rstanarm::student_t(df = 1)`  
2. To ensure reproducibility, we use `set.seed()`. This way we make sure that same random numbers are generated to be used during fitting procedure. The number `123` isn't special or related to our data; it is just a "seed" used to choose random numbers.     
3. Define our linear regression model specification while setting our engine as "stan" and provide the prior distribution information with arguments `prior` and `prior_intercept.    
4. Fit model with formula (`width ~ initial_volume * food_regime` ) and provide dataset (`urchins`) for fitting.


```{r go-stan, message = FALSE}
# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# make the parsnip model
bayes_mod <-   
  linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist) 

# train the model
bayes_fit <- 
  bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)

print(bayes_fit, digits = 5)
```

To update the parameter table, the `tidy()` method is once again used: 

```{r tidy-stan}
tidy(bayes_fit, intervals = TRUE)
```

A goal of the tidymodels packages is that the **interfaces to common tasks are standardized** (as seen in the `tidy()` results above). The same is true for getting predictions; we can use the same code even though the underlying packages use very different syntax:

```{r stan-pred}
bayes_plot_data <- 
  new_points %>% 
  bind_cols(predict(bayes_fit, new_data = new_points)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior distribution")
```

This isn't very different from the non-Bayesian results (except in interpretation). 

The [parsnip](https://parsnip.tidymodels.org/) package can work with many model types, engines and arguments. Check out [tidymodels.org/find/parsnip](https://www.tidymodels.org/find/parsnip/) to see what is available.

## Why does it work that way?

The extra step of defining the model using a function like `linear_reg()` might seem superfluous since a call to `lm()` is much more succinct. However, the problem with standard modeling functions is that they don't separate what you want to do from the execution. For example, the process of executing a formula has to happen repeatedly across model calls even when the formula does not change; we can't recycle those computations. 

Also, using the tidymodels framework, we can do some interesting things by incrementally creating a model (instead of using single function call). [Model tuning](/start/tuning/) with tidymodels uses the specification of the model to declare what parts of the model should be tuned. That would be very difficult to do if `linear_reg()` immediately fit the model. 

If you are familiar with the tidyverse, you may have noticed that our modeling code uses the magrittr pipe (`%>%`). With dplyr and other tidyverse packages, the pipe works well because all of the functions take the _data_ as the first argument. For example: 

```{r tidy-data}
urchins %>% 
  group_by(food_regime) %>% 
  summarize(med_vol = median(initial_volume))
```

whereas the modeling code uses the pipe to pass around the _model object_:

```{r tidy-model, eval = FALSE}
bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
```

This may seem jarring if you have used dplyr a lot, but it is extremely similar to how ggplot2 operates:

```{r eval=FALSE}
ggplot(urchins,
       aes(initial_volume, width)) +      # returns a ggplot object 
  geom_jitter() +                         # same
  geom_smooth(method = lm, se = FALSE) +  # same                    
  labs(x = "Volume", y = "Width")         # etc
```
